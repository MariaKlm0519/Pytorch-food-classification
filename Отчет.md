|<p>![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.001.png)**МИНОБРНАУКИ РОССИИ**</p><p>федеральное государственное бюджетное образовательное</p><p>учреждение высшего образования</p><p>**«Национальный исследовательский университет «МЭИ»**</p><p></p>|
| :-: |


|**Институт**|ИВТИ|
| :- | :-: |
|**Кафедра**|ПМИИ|

**ВЫПУСКНАЯ КВАЛИФИКАЦИОННАЯ РАБОТА**

**(БАКАЛАВРСКАЯ РАБОТА)**

|**Направление**|01\.03.02 Прикладная математика и информатика||||||||||||||
| :- | :-: | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- |
||(код и наименование)||||||||||||||
|**Образовательная программа**|Математическое и программное обеспечение вычислительных машин и компьютерных сетей||||||||||||||
|**Форма обучения**|очная||||||||||||||
||(очная/очно-заочная/заочная)||||||||||||||
|**Тема:**|Классификация блюд нейронными сетями||||||||||||||
|**Студент**|А-05-19||Калмыкова М.А.||||||||||||
||группа|подпись|фамилия и инициалы||||||||||||
|**Руководитель ВКР**|к.т.н.|доцент||Кружилов И.С.|||||||||||
||уч. степень|должность|подпись|фамилия и инициалы|||||||||||
||||||||||||||||
|организация|||||||||||||||
|**«Работа допущена к защите»**|||||||||||||||
|**Заведующий кафедрой**|к.т.н.|доцент||Варшавский П.Р.|||||||||||
||уч. степень|звание|подпись|фамилия и инициалы|||||||||||
||**Дата**||||||||||||||
||**Москва, 2023**||||||||||||||



**АННОТАЦИЯ**

Данная выпускная квалификационная работа (ВКР) посвящена исследованию возможностей нейронных сетей в задаче классификации изображений блюд. ВКР на тему "Классификация блюд нейронными сетями" авторства Калмыковой Марии Алексеевны состоит из 53 страниц, включая 22 иллюстрации. В работе были рассмотрены различные архитектуры нейронных сетей, а также методы предобработки данных. Была проведена обработка и анализ данных, включающих в себя фотографии блюд и их описания. Были получены результаты, показывающие эффективность использования нейронных сетей в задаче классификации блюд. Работа может быть использована в качестве основы для разработки приложений, основанных на распознавании блюд.


# **Оглавление**
[ВВЕДЕНИЕ	4****](#_toc139242079)**

[**ГЛАВА 1. АНАЛИЗ ПРИНЦИПОВ РАБОТЫ ИСКУССТВЕННЫХ НЕЙРОННЫХ СЕТЕЙ	7****](#_toc139242080)

[1.1 Обзор существующих подходов для решения задачи классификации изображений	7](#_toc139242081)

[1.2 Принцип работы искусственных нейронных сетей	9](#_toc139242082)

[1.3 Методы обучения искусственных нейронных сетей	14](#_toc139242083)

[1.4 Сверточные нейронные сети	19](#_toc139242084)

[**ГЛАВА 2. ОЦЕНКА КАЧЕСТВА РАБОТЫ АЛГОРИТМА КЛАССИФИКАЦИИ ИЗОБРАЖЕНИЙ С ПОМОЩЬЮ МЕТРИК	27****](#_toc139242085)

[2.1 Метрики бинарной классификации	27](#_toc139242086)

[2.2 Метрики многоклассовой классификации	32](#_toc139242087)

[**ГЛАВА 3. ПОДГОТОВКА ДАННЫХ ДЛЯ ОБУЧЕНИЯ	35****](#_toc139242088)

[3.1 Набор данных FOOD-101	35](#_toc139242089)

[3.2 Создание загрузчика данных для обучения нейронной сети	36](#_toc139242090)

[3.3 Аугментация данных для повышения эффективности обучения нейронной сети.	38](#_toc139242091)

[**ГЛАВА 4. ОБУЧЕНИЕ НЕЙРОННОЙ СЕТИ ДЛЯ КЛАССИФИКАЦИИ БЛЮД. ОЦЕНКА РЕЗУЛЬТАТОВ	41****](#_toc139242092)

[4.1 Перенос обучения	41](#_toc139242093)

[4.2 Обучение нейронной сети	44](#_toc139242094)

[4.3 Оценка результатов классификации	46](#_toc139242095)

[**ЗАКЛЮЧЕНИЕ	50****](#_toc139242096)

[**СПИСОК ЛИТЕРАТУРЫ	52****](#_toc139242097)



<a name="_toc138774238"></a>

<a name="_toc139242079"></a>**ВВЕДЕНИЕ**

Классификация блюд с помощью нейронных сетей является актуальной и важной темой с практическим применением в пищевой промышленности. Она может повысить эффективность служб доставки еды и помочь в лечении заболеваний и соблюдении диеты. Анализ изображений разных видов блюд позволяет классифицировать их по разным категориям и прогнозировать калорийность, что может быть полезно при лечении заболеваний и соблюдении диеты. Кроме того, классификация состояний объектов для приготовления пищи может быть полезна для распознавания активности того, что готовится, что улучшает понимание и предсказание.

Например, в одном исследовании была использована настроенная сверточная нейронная сеть VGG для классификации семи различных состояний пищевого продукта, включая такие как «целое», «выжатый сок», «нарезанный ломтиками», «нарезанный кубиками», «сливочная паста» и «тертый» [1]. Также, в другом исследовании, была использована сверточная нейронная сеть для классификации блюд тайского фаст-фуда на основе их изображений и прогноза калорийности блюд [2].

Классификация блюд может помочь в доставке, так как позволяет автоматически распознавать блюда на фотографиях и определять их название и состав, что может быть полезно для составления меню и управления запасами продуктов. Например, исследование показало, что использование классификации изображений блюд может помочь в автоматизации процесса составления меню для ресторанов и кафе [3]. В нем использовался набор данных, содержащий изображения блюд, разделенных на категории, такие как супы, салаты, мясные блюда и т.д. С помощью классификации изображений была разработана система, которая автоматически составляет меню на основе предпочтений клиентов и наличия продуктов на складе.

Таким образом, классификация блюд с помощью нейронных сетей имеет несколько потенциальных применений, которые могут повысить эффективность служб доставки еды, помочь в управлении медицинскими состояниями и соблюдением диеты, а также помочь в обнаружении фальсификации и порчи продуктов. Кроме того, классификация состояний продуктов для приготовления пищи может быть полезна для распознавания состояния готовности блюда.

Соответственно, актуальность работы на тему «классификация блюд с помощью нейронных сетей» определяется ростом популярности сервисов доставки еды, потребностью в точной информации о пищевой ценности, важностью контроля качества, возможностями персонализации. 

Целью данного исследования является разработка точных и эффективных моделей для классификации различных типов блюд на основе их изображений. Основная задача состоит в том, чтобы достичь высокой точности в классификации блюд при минимизации вычислительной мощности, необходимой для обучения.

В данной выпускной квалификационной работе (ВКР) исследуется вопрос о применении сверточных нейронных сетей для решения задачи классификации изображений. 

Объектом ВКР является классификация изображений по классам. 

Предмет ВКР: алгоритм классификации изображений с применением сверточных нейронных сетей. 

Целью работы является реализация алгоритма, способного классифицировать изображения 101 вида блюд. 

Структура выпускной квалификационной работы (ВКР) включает введение, анализ принципов работы и применения искусственных нейронных сетей для классификации изображений, выбор и реализацию алгоритма классификации изображений, обучение и оценку точности нейронной сети, заключение.

Первый раздел описывает основные принципы функционирования и обучения искусственных нейронных сетей, а также особенности сверточных нейронных сетей.

Во втором разделе рассматриваются основные метрики, используемые для оценки качества нейронных сетей в задачах классификации изображений.

Третий раздел посвящен реализации алгоритма классификации изображений по классам, основанного на сверточной нейронной сети, и также рассматривается вопрос об использовании аугментации данных для повышения точности модели.

В четвертом разделе проводится обучение входящей в состав алгоритма нейронной сети и оценивается точность реализованного алгоритма классификации.

В заключении делаются выводы по проведенной работе и предлагаются прогнозы относительно улучшения и применения реализованного алгоритма классификации изображений.

<a name="_toc138774239"></a><a name="_toc139242080"></a>**ГЛАВА 1. АНАЛИЗ ПРИНЦИПОВ РАБОТЫ ИСКУССТВЕННЫХ НЕЙРОННЫХ СЕТЕЙ**

<a name="_toc123170976"></a><a name="_toc135480006"></a><a name="_toc138774240"></a><a name="_toc139242081"></a>**1.1 Обзор существующих подходов для решения задачи классификации изображений** 

Классификация изображений является важной задачей в области компьютерного зрения и искусственного интеллекта, которая на сегодняшний день находится в центре внимания многих исследований. Она заключается в том, чтобы определить, к какому классу принадлежит данное изображение, используя набор признаков, которые были извлечены из него.

Существует несколько подходов к решению задачи классификации изображений, таких как метод опорных векторов, метод случайного леса, сверточные нейронные сети, глубокие сверточные нейронные сети и другие. Каждый из этих подходов имеет свои сильные и слабые стороны, а также свои особенности, которые следует учитывать при выборе подхода для конкретной задачи.

Метод опорных векторов основан на поиске гиперплоскости в пространстве признаков, которая разделяет классы таким образом, чтобы максимизировать расстояния между гиперплоскостью и ближайшими объектами классов [14]. Он может использоваться для решения задач как бинарной, так и многоклассовой классификации. Для бинарной классификации SVM строит гиперплоскость, которая разделяет два класса, в то время как для многоклассовой классификации SVM строит несколько гиперплоскостей, каждая из которых разделяет пару классов [14]. Одним из преимуществ метода опорных векторов является его способность обрабатывать большой объем данных и работать с большим количеством признаков. Однако весомым недостатком метода является его неустойчивость к шуму в исходных данных, что зачастую встречается на реальных фотографиях. Объекты-выбросы являются опорными и существенно влияют на результат обучения.

Метод случайного леса является ансамблем решающих деревьев, который строит несколько деревьев на обучающей выборке и принимает решение путем голосования [5]. Каждое дерево выбирает случайный набор признаков и строит решающее дерево на основе этих признаков. Затем все деревья голосуют за класс, который имеет наибольшее количество голосов [5]. 

Одним из преимуществ метода случайного леса является его способность работать с неструктурированными данными, такими как изображения. Кроме того, этот метод может работать с большими объемами данных и обеспечивать высокую точность классификации. Недостатком метода случайного леса является его неэффективность при работе с очень большим количеством признаков.

Однако, для задачи классификации изображений, особенно для классификации изображений блюд, наиболее эффективным методом является использование сверточных нейронных сетей, так как они специализируются на обработке изображений и могут эффективно извлекать признаки, необходимые для точной классификации.

Сверточные нейронные сети (CNN) являются одним из наиболее используемых методов для классификации изображений. CNN – это тип нейронных сетей, которые специализированы на анализе и обработке изображений. Они являются эффективным способом решения задач классификации, так как они могут автоматически извлекать признаки из изображений и использовать их для принятия решений о классификации.

Основным компонентом CNN являются сверточные слои, которые содержат набор фильтров, называемых ядрами свертки [15]. Каждый фильтр сканирует изображение и выделяет определенные признаки, такие как границы, углы, текстуры и т. д. Эти сверточные слои могут использоваться для построения более высокоуровневых признаков, таких как объекты и сцены.

Одним из преимуществ CNN является их способность автоматически извлекать признаки из изображений, что позволяет достичь высокой точности классификации. Кроме того, CNN могут работать с различными типами изображений, включая черно-белые, цветные, градиентные и т. д. Недостатком CNN является их высокая сложность и большое количество параметров, что требует высокой вычислительной мощности для обучения и использования.

<a name="_toc123170977"></a><a name="_toc135480007"></a><a name="_toc138774241"></a><a name="_toc139242082"></a>**1.2 Принцип работы искусственных нейронных сетей**

Искусственная нейронная сеть – это математическая модель, которая имитирует работу нейронов головного мозга и используется для решения различных прикладных задач. Она состоит из множества искусственных нейронов, которые являются простыми однотипными вычислительными элементами. Каждый искусственный нейрон в такой нейронной сети является функцией, которая принимает на вход значения, умноженные на весовые коэффициенты, и выполняет вычисления с использованием этих параметров. Результаты произведения суммируются и проходят через нелинейную функцию, которая является показателем силы возбуждения нейрона [13]. Искусственные нейронные сети объединяют в себе множество таких нейронов и позволяют решать ряд задач, таких как регрессионный анализ, классификация, кластеризация, распознавание речи и машинный перевод. Они обладают способностью к обучению и свойством нелинейности, что позволяет находить сложные зависимости между данными и учитывать опыт для улучшения результата работы [13].

Структура искусственной нейронной сети организована в виде направленного графа, где нейроны являются узлами, а весовые коэффициенты определяют связи между ними.

Данная структура представлена на рисунке 1.1.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.002.png)

Рисунок 1.1. Структура искусственной нейронной сети

Для понимания принципа работы искусственной нейронной сети можно рассмотреть персептрон – одну из первых моделей нейронной сети (рис. 1.2).

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.003.png)

Рисунок 1.2. Модель персептрона

` `Между входными нейронами, которые получают сигнал, и сумматором в нейронной сети существуют связи с весовыми коэффициентами. Каждая связь имеет свой вес, который определяет, насколько значимым является соответствующий входной сигнал для формирования выходного значения нейрона. Так, весовые коэффициенты позволяют настраивать и регулировать влияние каждого входа на итоговый результат вычислений нейрона. Сумматор вычисляет взвешенную сумму, складывая входные сигналы, умноженные на свои весовые коэффициенты. Полученная сумма подается на функцию активации, которая определяет выходное значение нейрона [16].

Функция активации играет важную роль в работе нейронной сети, определяя, должен ли нейрон быть активирован. Она нормализует сигнал, проходящий через нее, и представляет результат работы нейронной сети в нужном диапазоне в зависимости от типа функции активации [13].

Если в нейронной сети отсутствует функция активации, то её архитектура будет сводиться к простой модели линейной регрессии. Таким образом, функция активации играет важную роль, поскольку она позволяет нейронной сети выполнять нелинейные преобразования для входных данных. Благодаря этому, нейронная сеть может решать более сложные задачи, обнаруживать нелинейные зависимости и извлекать высокоуровневые признаки из данных. Функции активации, такие как сигмоида, гиперболический тангенс или ReLU, добавляют нелинейность и позволяют нейронной сети обучаться и адаптироваться к различным типам данных и задачам.

Существует множество различных функций активации, таких как ступенчатая, гиперболический тангенс, линейная, функция усеченного линейного преобразования (ReLU), сигмоидальная, которые представлены на рисунке 1.3. 

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.004.png)

Рисунок 1.3. Основные функции активации

Каждая из этих функций активации имеет свои особенности и может быть использована для определенных задач.

Функция активации выбирается в зависимости от типа задачи, которую решает нейронная сеть. 

Персептрон – это простейший вид нейронной сети, который может решать только простые задачи классификации и не предназначен для решения более сложных задач. Для решения более сложных задач были разработаны многослойные нейронные сети прямого распространения. Эти сети состоят из нейронов, организованных в слои. В многослойных нейронных сетях каждый нейрон в слое соединен с нейронами следующего слоя, но не имеет связей с нейронами в том же слое. Первый слой называется входным, а последний слой — выходным. Все промежуточные слои, расположенные между входным и выходным слоями, называются скрытыми слоями.

В большинстве архитектур многослойных нейронных сетей важным элементом является смещение, которое присутствует во входном и скрытых слоях. Смещение представляет собой дополнительный параметр, добавляемый к взвешенной сумме входных значений нейрона перед применением функции активации. Он позволяет сети смещаться относительно идеальной гиперплоскости и улучшает ее способность аппроксимировать сложные нелинейные функции. Смещение позволяет нейронным сетям обучаться более гибким и адаптивным способом, улучшая их способность к обобщению и повышая их эффективность в решении различных задач. Многослойная нейронная сеть, включающая нейроны смещения, представлена на рисунке 1.4.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.005.png)

Рисунок 1.4. Многослойная нейронная сеть с нейронами смещения

Таким образом, существует разнообразие архитектур нейронных сетей, которые отличаются количеством скрытых слоев, числом нейронов в каждом слое и типами связей между нейронами. Каждая из этих архитектур имеет свои преимущества в решении определенных типов задач. Однако для того, чтобы нейронная сеть могла успешно решать задачи различного характера, ее необходимо обучить путем подбора оптимальных параметров и оптимизации функции ошибки. Обучение нейронной сети заключается в подстройке весов и смещений нейронов таким образом, чтобы минимизировать ошибку между предсказанными и фактическими значениями. Этот процесс требует большого объема данных для тренировки и использования алгоритмов оптимизации, таких как градиентный спуск, для обновления параметров сети.

<a name="_toc138774242"></a><a name="_toc139242083"></a>**1.3 Методы обучения искусственных нейронных сетей**

Искусственные нейронные сети обладают важной особенностью – обучаемостью. Основная цель обучения заключается в минимизации ошибки между выходными значениями сети, полученными при прямом распространении ошибки, и ожидаемыми значениями. Процесс получения выходных данных нейронной сети на основе одного экземпляра тренировочных данных называется прямым распространением ошибки [11]. Для вычисления ошибки искусственной нейронной сети используется функция потерь. Среднеквадратичное отклонение и среднее абсолютное отклонение являются наиболее распространенными функциями потерь. Данные функции потерь представлены в формуле (1.1).

|<p>MSE: y'-y2,</p><p>MAE:y'-y, </p>|(1.1)|
| :-: | :-: |

где y' – ожидаемое значение;

y – результат работы сети.

Для обучения нейронных сетей используются различные методы обучения, которые можно разделить на стохастические и детерминированные. Стохастические методы основаны на случайных корректировках весовых коэффициентов сети, причем эти изменения сохраняются только в случае, если они приводят к уменьшению ошибки. Такие методы обычно используются в стохастическом градиентном спуске, где веса обновляются на основе градиента функции ошибки, вычисленного на небольших случайно выбранных подмножествах данных, называемых мини-пакетами или пакетами. Это позволяет эффективно обучать модель на больших наборах данных.

Детерминированные методы, наоборот, стремятся точно определить и скорректировать только те весовые коэффициенты, которые имеют наибольшее влияние на ошибку. Они обычно используются в методах, таких как обратное распространение ошибки, где градиенты вычисляются для всех весовых коэффициентов сети, а затем веса обновляются с использованием этих градиентов. Детерминированные методы могут быть более точными, но могут также требовать больше вычислительных ресурсов и времени для обучения на больших наборах данных.

Выбор между стохастическими и детерминированными методами зависит от конкретной задачи и доступных ресурсов. Оба вида методов имеют свои преимущества и недостатки, и выбор должен основываться на требованиях и ограничениях конкретного проекта.

Для обучения нейронных сетей в основном используется детерминированный метод, называемый градиентным спуском. Этот метод оптимизации позволяет минимизировать функцию потерь нейронной сети путем итеративной корректировки весовых коэффициентов. Принцип градиентного спуска на примере одного весового коэффициента представлен на рисунке 1.5.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.006.png)

Рисунок 1.5. Принцип градиентного спуска

Обучение по алгоритму градиентного спуска заключается в поиске градиента функции f в конкретной точке, которая является текущей ошибкой нейронной сети, как показано на рисунке 1.5. Градиент указывает направление наискорейшего возрастания функции, а соответственно антиградиент – направление наискорейшего убывания, поэтому для минимизации ошибки необходимо сдвигать значение весового коэффициента в сторону него [4]. Тогда обновление весового коэффициента по методу градиентного спуска вычисляется по формуле (1.2):

|w+= w-η\*∇E,|(1.2)|
| :-: | :-: |

где w+ – новое значение весового коэффициента;

w – предыдущее значение весового коэффициента;

∇E – градиент функции ошибки по весовому коэффициенту;

η – скорость обучения, определяющая величину шага обновления.

Если выбрать слишком маленькую скорость обучения, то минимизация функции потерь может занять слишком много времени; с другой стороны, если выбрать слишком большое значение, то можно перешагнуть минимум и не достичь его [4].

На рисунке 1.5 иллюстрируется зависимость ошибки от одного весового коэффициента для простой модели или однослойной нейронной сети. Однако в многослойных нейронных сетях количество весовых коэффициентов значительно возрастает и может достигать десятков, сотен или даже тысяч. Это связано с тем, что в каждом слое многослойной сети каждый нейрон связан с нейронами предыдущего слоя, и каждая связь имеет свой весовой коэффициент. Таким образом, общее количество весовых коэффициентов в многослойной сети определяется числом нейронов в каждом слое и их связей. Учет всех этих весовых коэффициентов является важной задачей при обучении и оптимизации многослойных нейронных сетей.

Для минимизации функции потерь в такой сети необходимо рассчитать градиенты для всех весовых коэффициентов, то есть их частные производные. Для этого применяется метод обратного распространения ошибки, который включает вычисление частных производных по каждому весовому коэффициенту. Это позволяет определить, как каждый из этих весов вносит вклад в общую ошибку [13].

Процесс начинается с вычисления ошибки на выходном слое, которая затем передается на предыдущий скрытый слой. Затем ошибка этого скрытого слоя передается на следующий слой, и так далее. Эта операция повторяется для всех слоев, начиная с последнего и заканчивая первым. Таким образом, мы находим производные потерь по весам каждого слоя, определяя, в каком направлении необходимо регулировать веса для уменьшения общих потерь.

Вычисление ошибки для весовых коэффициентов последнего и остальных слоев происходит по-разному. Вклад весовых коэффициентов последнего слоя в общую ошибку рассчитывается с использованием формулы (1.3).

|*¶* E*¶* wj,k=*¶* ik*¶* wj,k\**¶* ok *¶* ik\**¶* E*¶* ok=oj\**¶* fk(sk) *¶* sk\**¶* j,|(1.3)|
| :-: | :-: |

где wj,k – весовой коэффициент, соединяющий нейроны j и k;

*¶* E*¶* wj,k – вклад веса wj,k в общую ошибку;

*¶* ik*¶* wj,k – вклад веса wj,k во взвешенную сумму нейрона k;

*¶* ok *¶* ik – вклад взвешенной суммы в выходной сигнал нейрона k;

*¶* E*¶* ok – вклад входного сигнала нейрона k в общую ошибку;

oj – выходной сигнал нейрона j;

*¶* fk(sk) *¶* sk – производная функции активации нейрона j;

*¶* j – ошибка выходного сигнала нейрона j, находящаяся с помощью функции потерь.

Вычисление вклада весовых коэффициентов всех слоев, за исключением последнего, в общую ошибку осуществляется согласно формуле (1.4).

|*¶* E*¶* wj,k=*¶* ik*¶* wj,k\**¶* ok *¶* ik\*l*¶* l+1*¶* ok,|(1.4)|
| :-: | :-: |

где *¶* l+1*¶* ok – вклад выходного сигнала нейрона k в ошибку выходного сигнала нейрона l на следующем слое, стоящим за слоем нейрона k.

Данный дифференциал вычисляется как умножение весового коэффициента wk,l на произведение *¶* ok *¶* ik\**¶* E*¶* ok, вычисляемое в формуле (1.3).

Обучение нейронной сети с помощью градиентного спуска происходит путем итеративной корректировки весовых коэффициентов. В каждой итерации процесса настройки весов выполняются следующие шаги:

1\. Производится прямое распространение ошибки для каждого примера тренировочных данных, и с помощью функции потерь вычисляется ошибка.

2\. Вычисляется сумма квадратов всех ошибок.

3\. Значения весовых коэффициентов нейронной сети корректируются с помощью обратного распространения ошибки.

Градиентный спуск имеет недостаток в том, что настройка весовых коэффициентов и обучение модели требуют значительного времени, особенно при больших объемах тренировочных данных. Для преодоления этой проблемы используется модифицированный алгоритм градиентного спуска, известный как стохастический градиентный спуск.

Стохастический градиентный спуск отличается тем, что корректировка весов происходит на основе каждого отдельного образца или небольшой группы образцов из тренировочного набора данных. Хотя это приводит к неточной корректировке весов из-за использования только части данных, функция потерь минимизируется не плавно, а скачками, но при этом сходится к локальному минимуму гораздо быстрее.

На рисунке 1.6 показано сравнение процесса обучения с использованием различных методов градиентного спуска.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.007.png)

Рисунок 1.6. Процесс обучения при использовании различных вариантов градиентного спуска

Одной из главных проблем, связанных с алгоритмом градиентного спуска, является возможность нахождения локального минимума функции вместо глобального при большом количестве весовых коэффициентов. Для того чтобы избежать этой проблемы, в начале обучения весовые коэффициенты устанавливаются случайным образом, чтобы найти новый минимум функции и дополнительно минимизировать функцию потерь. Если функция потерь не уменьшается, то необходимо внести изменения в структуру искусственной нейронной сети.

Таким образом, мы рассмотрели основной детерминированный метод обучения нейронных сетей, называемый градиентным спуском. Однако для того, чтобы научить нейронную сеть распознавать образы на изображении, необходимо использовать специальную архитектуру искусственной нейронной сети, называемую сверточной.

<a name="_toc123170980"></a><a name="_toc135480009"></a><a name="_toc138774243"></a><a name="_toc139242084"></a>**1.4 Сверточные нейронные сети**

Сверточная нейронная сеть является одной из самых популярных архитектур нейронных сетей, применяемых для анализа визуальных образов и классификации изображений. Ее основа заключается в идее зрительной коры головного мозга, где отдельные группы клеток реагируют на стимулы в своих специфических областях зрительного поля, называемых рецептивными полями. При восприятии различных образов в зрительной коре активируются разные группы нейронов. Архитектура сверточной нейронной сети позволяет выделять в данных различные признаки, начиная от очень простых и заканчивая более сложными.

Архитектура сверточной нейронной сети включает в себя чередующиеся слои свертки и подвыборки, за которыми следуют полносвязные слои. Рисунок 1.7 демонстрирует данную архитектуру.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.008.png)

Рисунок 1.7. Архитектура свёрточной нейронной сети

Сверточный слой принимает на вход изображение в виде матрицы пикселей, размерность которой зависит от размера изображения. Если изображение черно-белое, то на вход подается одна матрица, где каждый пиксель описывается значением яркости в диапазоне от 0 до 255. В случае цветных изображений используются три матрицы, каждая из которых отвечает за один из каналов цветовой модели RGB, значения матриц описывают интенсивность цвета пикселей. Ядро свертки, которое выступает в качестве нейрона, представляет собой матрицу весовых коэффициентов . Глубина ядра свертки должна быть такой же, как и глубина изображения. Для цветных изображений используется ядро свертки, состоящее из трех матриц, каждая из которых применяется для свертки по одному из каналов цветного изображения. Размерность матрицы ядра свертки обычно варьируется от 3 до 7 и определяет количество признаков, которые необходимо объединить в один новый признак более высокого уровня. Операция свертки начинается с определения рецептивного поля, размер которого равен размеру ядра свертки. Рецептивное поле скользит с некоторым смещением по исходному изображению, и пиксели изображения, попавшие в него, поэлементно перемножаются с ядром свертки. Элементы новой матрицы, полученной в результате перемножения, суммируются и образуют новый выходной пиксель. Операция свертки проиллюстрирована на рисунке 1.8.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.009.png)

Рисунок 1.8. Операция свертки

Операция свертки для обработки изображений может быть выполнена как для черно-белых, так и для цветных изображений. На рисунке 1.8 показана операция свертки для черно-белого изображения, в то время как на рисунке 1.9 представлен процесс операции свертки для цветного изображения. Отличие заключается в том, что для цветного изображения каждый цветовой канал обрабатывается отдельно с помощью одной из матриц, составляющих трехмерное ядро свертки, после чего полученные матрицы суммируются в одну результирующую матрицу.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.010.png)

Рисунок 1.9. Операция свертки цветного изображения

При выполнении операции свертки исходное изображение подвергается обработке, в результате чего получается матрица с меньшей размерностью. Это происходит потому, что граничные пиксели исходного изображения не оказываются в центре рецептивного поля. Чтобы сохранить информацию о границах изображения и избежать постоянного сжатия изображения при выполнении свертки, вокруг изображения добавляется рамка из нулей [15]. Ширина этой рамки зависит от размера ядра свертки, а сама операция называется заполнением (zero-padding). Пример операции свертки цветного изображения с заполнением представлен на рисунке 1.10.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.011.png)

Рисунок 1.10. Операция свертки цветного изображения с применением заполнения

В сверточных нейронных сетях, аналогично многослойным нейронным сетям, присутствует смещение, значение которого изменяется в процессе обучения и добавляется к каждому элементу выходной матрицы. Затем полученная матрица проходит через функцию активации, прежде чем поступить на следующий слой. Обычно в сверточных слоях используется функция усеченного линейного преобразования в качестве функции активации. Эта функция преобразует все отрицательные значения в 0, оставляя остальные значения без изменений. Функция усеченного линейного преобразования была представлена на рисунке 1.3.

После выполнения операции свертки на изображении формируется матрица, которая называется картой признаков. На этой карте признаков выделяются различные признаки, такие как прямые и кривые линии, полуокружности или более сложные признаки, в зависимости от того, где расположен сверточный слой в модели нейронной сети. Если первый сверточный слой выделяет базовые признаки, то последующие слои выделяют признаки все более высокого уровня, обобщая информацию на изображении [15]. Обычно в сверточных слоях используется более одного ядра свертки, каждое из которых идентифицирует конкретный признак. Поэтому в результате свертки получается набор различных числовых матриц, число которых характеризует глубину карты признаков. Чем больше ядер свертки используется, тем более подробную информацию можно получить об исходном изображении.

После слоя свертки в модели нейронной сети обычно следует слой подвыборки, который уменьшает размерность карты признаков. Это достигается путем замены некоторой области пикселей одним пикселем. Такой подход позволяет сохранить информацию о наличии конкретных признаков на изображении, но не их местоположение, и снизить вычислительную нагрузку, уменьшив количество параметров сверточной сети [15]. Для выполнения операции подвыборки обычно используются два метода: по среднему значению и по максимальному значению. Однако чаще всего используют подвыборку по максимальному значению, так как она более четко выделяет ключевые признаки, подавляя лишние шумы на изображении [15].

При выполнении операции подвыборки учитываются два параметра: размер ядра подвыборки и размер шага в пикселях, на которое сдвигается ядро подвыборки. Чаще всего используют пару значений параметров: 2x2, которая уменьшает размерность карты признаков в два раза. Пример выполнения операции можно увидеть на рисунке 1.11. Однако перед выполнением необходимо выполнить слой свертки, чтобы получить карту признаков.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.012.png)

Рисунок 1.11. Пример операции подвыборки по максимальному значению

Для выявления сложных признаков на изображении сверточные сети содержат множество чередующихся слоев свертки и подвыборки. С каждой парой слоев свертки глубина карты признаков изменяется в зависимости от числа ядер свертки, а их размерность уменьшается.

Конечными слоями сверточной нейронной сети являются полносвязные слои, с помощью которых происходит классификация объекта на изображении. В процессе обучения весовые коэффициенты данных слоев настраиваются для определения метки класса на основе выявленных признаков [13].

Перед подачей полученной карты признаков на полносвязный слой, слои карты признаков объединяются в вектор, где каждый элемент показывает вероятность присутствия конкретного признака на изображении. Например, для классификации изображения как лица, вероятность присутствия таких признаков, как глаза, рот и нос, должна быть высокой.

Последний полносвязный слой содержит количество нейронов, равное количеству определяемых классов, и с помощью выбранной функции активации выводит вероятности присутствия каждого класса на изображении. Это означает, что каждый нейрон на выходе слоя соответствует конкретному классу, и его значение показывает вероятность того, что объект на изображении принадлежит этому классу.

Таким образом, полносвязный слой является ключевым элементом сверточной нейронной сети, который позволяет классифицировать объекты на изображениях. Он принимает на вход вектор признаков и настраивает свои весовые коэффициенты для определения метки класса.

<a name="_toc123170981"></a><a name="_toc135480010"></a><a name="_toc138774244"></a><a name="_toc139242085"></a>**ГЛАВА 2. ОЦЕНКА КАЧЕСТВА РАБОТЫ АЛГОРИТМА КЛАССИФИКАЦИИ ИЗОБРАЖЕНИЙ С ПОМОЩЬЮ МЕТРИК**

Оценка работы нейронной сети – это важный этап в процессе разработки и использования модели машинного обучения. Для оценки качества работы нейросети используются различные метрики, которые позволяют оценить точность, полноту, специфичность и F-меру алгоритма классификации.

Метрика – это функция, измеряющая качество прогнозов модели при использовании контрольной выборки, результаты которой выводятся в конце каждой эпохи обучения [10, с. 58]. 

Каждая метрика характеризует определенный аспект работы алгоритма и может быть использована для определения того, насколько точно алгоритм классифицирует изображения. 

<a name="_toc139242086"></a>**2.1 Метрики бинарной классификации**

В задаче бинарной классификации для каждого объекта в выборке возможны 4 ситуации:

- Верно предсказанная положительная метка. Группа true positive TP (true – потому что предсказали мы правильно, а positive – потому что предсказали положительную метку);
- Неверно предсказанная положительная метка (предсказана положительная метка для отрицательной метки). Группа false positive FP (false, потому что предсказание было неправильным);
- Верно предсказанная отрицательная метка. Группа true negative (TN);
- Неверно предсказанная отрицательная метка (предсказана отрицательная метка для положительной метки). Группа false negative (FN). 

Для удобства все эти показателя изображают в виде таблицы, которую  и называют матрицей ошибок. Данная матрица представлена на рисунке 2.1.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.013.png)

Рисунок 2.1. Матрица ошибок

Рассмотрим такие основные метрики, как точность (accuracy), точность предсказания положительного класса (precision), полнота (recall), F1-score, ROC-кривая и AUC.

- Точность (accuracy) 

Точность (accuracy) – это метрика, которая показывает, насколько точно алгоритм классифицирует изображения. Она определяется как отношение числа правильно классифицированных изображений к общему числу изображений в тестовой выборке. Точность может быть вычислена по следующей формуле: 

|accuracy =TP + TNTP + TN + FP + FN,|(2.1)|
| :-: | :-: |

где TP (true positive) – число правильно классифицированных изображений положительного класса; 

TN (true negative) – число правильно классифицированных изображений отрицательного класса;

FP (false positive) – число неправильно классифицированных изображений положительного класса;

FN (false negative) – число неправильно классифицированных изображений отрицательного класса. 

Например, если алгоритм правильно классифицирует 90 из 100 изображений, то его точность будет равна 0,9 (или 90%). Однако точность может быть недостаточно информативной метрикой в случае несбалансированных классов, когда один класс представлен значительно большим количеством изображений, чем другой. В таком случае, точность может быть высокой, но алгоритм может плохо работать на классе с меньшим количеством изображений. 

- Точность предсказания положительного класса (precision) 

Точность предсказания положительного класса (precision) – это метрика, которая показывает, насколько точно алгоритм предсказывает положительный класс. Она определяется как отношение числа правильно классифицированных изображений положительного класса к общему числу изображений, которые алгоритм предсказал как положительный класс. Точность предсказания положительного класса может быть вычислена по следующей формуле: 

|precision = TPTP + FP|(2.2)|
| :-: | :-: |

Например, если алгоритм предсказывает 100 изображений как положительный класс, из которых 80 действительно являются положительным классом, а 20 – отрицательным классом, то его точность предсказания положительного класса будет равна 0,8 (или 80%). 

Преимущества точности: простота и понятность метрики, интерпретируемость (показывает долю правильно классифицированных положительных примеров), учет ошибок второго рода (ложноположительных предсказаний).

Недостатки точности: не учитывает ошибки первого рода (не учитывает случаи, когда модель неправильно классифицирует положительные примеры как отрицательные), чувствительна к дисбалансу классов (может быть искажена, если классы несбалансированны).

- Полнота (recall) 

Полнота (recall) – это метрика, которая показывает, насколько хорошо алгоритм находит все положительные изображения. Она определяется как отношение числа правильно классифицированных изображений положительного класса к общему числу изображений положительного класса в тестовой выборке. Полнота может быть вычислена по следующей формуле: 

|recall =TPTP + FN|(2.3)|
| :-: | :-: |

Например, если в тестовой выборке есть 100 изображений положительного класса, из которых алгоритм правильно классифицирует 80, а 20 остаются неклассифицированными, то его полнота будет равна 0,8 (или 80%). 

Главное преимущество метрики заключается в том, что она учитывает ошибки первого рода и показывает способность модели обнаруживать все положительные примеры. Это особенно полезно в задачах, где критично минимизировать пропуск целевого класса, например, при обнаружении заболеваний. Полнота помогает идентифицировать все истинно положительные случаи, что может быть критически важно для принятия решений.

Главный недостаток метрики recall заключается в том, что она не учитывает ошибки второго рода. Это означает, что recall не учитывает случаи, когда модель неправильно классифицирует отрицательные примеры как положительные. Это может привести к неправильной оценке производительности модели, особенно если ложные срабатывания нежелательны. Также следует отметить, что метрика recall может быть искажена, если классы несбалансированы, и в таких случаях может быть полезно использовать дополнительные метрики для более полной оценки модели.

- F1-score 

F1-score – это метрика, которая является средним гармоническим между точностью и полнотой. Она позволяет учитывать как точность, так и полноту при оценке качества работы алгоритма. F1-score может быть вычислен по следующей формуле: 

|F1-score = 2 \*precision \* recallprecision + recall|(2.4)|
| :-: | :-: |

Например, если точность предсказания положительного класса равна 0,8, а полнота – 0,9, то F1-score будет равен 0,84. 

Оценка F1-score варьируется от 0 до 1, причем 1 – это наилучший возможный результат. Этот показатель эффективен для использования, когда классы несбалансированны, поскольку он учитывает как ложноположительные, так и ложноотрицательные результаты.

- ROC-кривая и AUC 

ROC-кривая (receiver operating characteristic curve) – это кривая, которая показывает зависимость между долей истинно положительных классификаций (True Positive Rate, TPR) и долей ложноположительных классификаций (False Positive Rate, FPR) при изменении порога бинарной классификации. 

ROC-кривая может быть построена путем изменения порога бинарной классификации и вычисления TPR и FPR для каждого порога. После этого, TPR и FPR могут быть использованы для построения ROC-кривой.

AUC (area under the curve) – это площадь под ROC-кривой. Чем выше AUC, тем лучше работает алгоритм классификации изображений. 

Например, если AUC равен 0,9, то это означает, что алгоритм классификации изображений работает хорошо, и его предсказания можно доверять. 

Главное преимущество ROC-кривой и ее показателя AUC заключается в их способности оценивать производительность модели в условиях несбалансированных классов и различных пороговых значений.

ROC-кривая представляет собой график, который отображает отношение между долей истинно положительных TPR и долей ложно положительных FPR результатов при изменении порогового значения. AUC, в свою очередь, представляет собой площадь под ROC-кривой и показывает общую производительность модели.

Главный недостаток ROC-кривой и AUC заключается в том, что они не учитывают ошибки второго рода (ложно отрицательные результаты). Они фокусируются только на различии между истинно положительными и ложно положительными результатами. Поэтому, если важно учесть ошибки второго рода, необходимо использовать дополнительные метрики, такие как precision (точность) и recall (полнота).

Кроме того, ROC-кривая и AUC могут быть неинформативными в случае сильно сбалансированных классов или когда интересует точность предсказаний на конкретном пороговом значении. В таких случаях может быть полезно рассмотреть другие метрики.

Таким образом, каждая метрика имеет свои преимущества и недостатки и может быть использована для определения того, насколько точно алгоритм классифицирует изображения. 

<a name="_toc139242087"></a>**2.2 Метрики многоклассовой классификации**

Многоклассовая классификация – это процесс разделения данных на более чем два класса. В отличие от метрик бинарной классификации, метрики многоклассовой классификации учитывают не только правильность классификации, но и количество классов.

При оценке качества работы алгоритмов многоклассовой классификации используются различные метрики. Одной из наиболее распространенных метрик является макро-усреднение (macro-averaging). При использовании этой метрики вычисляются показатели, такие как точность, полнота и F-мера, для каждого класса отдельно. Затем эти показатели усредняются по всем классам. Такой подход позволяет учитывать несбалансированность классов, поскольку каждый класс вносит равный вклад в итоговую метрику. Если один класс имеет большую долю в данных, макро-усреднение гарантирует, что этот класс не будет иметь сильное влияние на итоговую оценку качества.

Другой распространенной метрикой многоклассовой классификации является микро-усреднение (micro-averaging). При использовании этой метрики вычисляются показатели, такие как точность, полнота и F-мера, для каждого объекта отдельно. Затем эти показатели усредняются по всем объектам. Такой подход учитывает сбалансированность классов, поскольку каждый объект вносит равный вклад в итоговую метрику. Микро-усреднение особенно полезно, когда классы имеют примерно одинаковое количество объектов.

Кроме того, существуют метрики, которые учитывают не только правильность классификации, но и вероятности принадлежности объектов к классам. Например, метрика log loss (логарифмическая функция потерь) учитывает вероятности принадлежности объектов к классам и штрафует алгоритм за неверные прогнозы с высокой уверенностью. Это особенно полезно, когда важна не только точность классификации, но и уверенность алгоритма в своих прогнозах.

Выбор конкретных метрик многоклассовой классификации зависит от конкретной задачи и требований к качеству классификации. Например, если важно учесть несбалансированность классов, то макро-усреднение может быть предпочтительным, так как оно учитывает вклад каждого класса независимо от его размера. Если же классы сбалансированы, то микро-усреднение может быть более подходящим, так как оно учитывает каждый объект равномерно. Также можно комбинировать различные метрики для получения более полной оценки качества работы алгоритма.

В целом, использование метрик многоклассовой классификации позволяет более точно оценить качество работы алгоритмов классификации, учитывая особенности многоклассовых задач и требования к классификации. Это помогает исследователям и практикам в выборе наиболее подходящего алгоритма и настройке его параметров для достижения наилучших результатов.

<a name="_toc139242088"></a>**ГЛАВА 3. ПОДГОТОВКА ДАННЫХ ДЛЯ ОБУЧЕНИЯ**

<a name="_toc139242089"></a>**3.1 Набор данных FOOD-101**

В задаче классификации изображений блюд, выбор подходящего набора обучающих данных играет ключевую роль. Несмотря на то, что в последние годы было создано множество подобных наборов, не все они подходят для решения данной задачи. В данном параграфе мы рассмотрим набор данных FOOD-101, который является одним из наиболее подходящих для классификации изображений блюд.

Набор данных FOOD-101 был создан в 2014 году и содержит более 100 тысяч изображений 101 различной категории блюд. Каждая категория содержит примерно 1000 изображений, сделанных в различных условиях освещения, углах и масштабах.

Разнообразие кухонь, представленных в FOOD-101, а также высокое качество изображений и широкий спектр блюд, включая десерты, напитки и закуски, делают его применимым во многих сферах и позволяют удовлетворить большинство потребностей различных приложений, таких как поисковики ресторанов или фитнес-приложения. Кроме того, в наборе данных содержится много изображений для каждой категории, что повышает точность алгоритмов классификации, обученных на этих изображениях.

Значимым аспектом FOOD-101 является то, что изображения были собраны из интернета, а не были профессионально сфотографированы. Это означает, что изображения содержат различные артефакты, такие как шум, размытость, изменение цвета и т.д., что соответствует несовершенному качеству анализируемых фотографий в реальных условиях и проектах.

Для работы с набором данных FOOD-101 необходимо сначала загрузить его и разделить на обучающую и тестовую выборки. Обычно для обучения модели используется 70-80% изображений, а оставшиеся 20-30% используются для тестирования.

В данной работе для загрузки данных используется модуль Datasets из библиотеки Torchvision.

На рисунке 3.1 представлены примеры изображений, которые входят в обучающий набор данных, а на рисунке 3.2 – примеры изображений из тестового набора данных.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.014.png)

Рисунок 3.1. Пример изображений из тренировочного набора данных

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.015.png)

Рисунок 3.2. Пример изображений из тестового набора данных

<a name="_toc139242090"></a>**3.2 Создание загрузчика данных для обучения нейронной сети**

Dataloader (загрузчик данных) – это компонент, который используется для эффективной загрузки и предварительной обработки данных во время обучения нейронных сетей. Он выполняет несколько важных функций:

1\. Загрузка данных: Dataloader позволяет загружать данные из различных источников, таких как файлы на диске, базы данных или веб-сервисы. Он упрощает процесс чтения и представления данных в формате, который можно использовать для обучения модели.

2\. Пакетная обработка: Вместо загрузки и обработки данных по одному примеру за раз, Dataloader позволяет группировать данные в пакеты (batch) определенного размера. Это позволяет эффективно использовать вычислительные ресурсы и ускоряет обучение модели, так как многие операции, такие как вычисление градиентов, могут быть выполнены параллельно на пакетах данных.

3\. Перемешивание данных: Часто желательно перемешивать данные перед каждой эпохой обучения, чтобы модель не запоминала порядок примеров. Dataloader позволяет автоматически перемешивать данные перед каждой эпохой, что способствует лучшей обобщающей способности модели.

4\. Предварительная обработка данных: Dataloader может выполнять предварительную обработку данных перед их использованием для обучения модели. Например, он может применять нормализацию, изменение размера изображений или преобразование данных в числовой формат, понятный для модели.

5\. Распределение данных на устройства: Dataloader позволяет эффективно распределить данные на доступные вычислительные устройства, такие как графические процессоры (GPU). Это особенно полезно при обучении глубоких нейронных сетей, которые требуют больших вычислительных мощностей.

Загрузчик данных настраивается в соответствии с типом и форматом данных, используемых в нейронной сети, а также с учетом требований к обработке данных. Он может быть написан как на языке Python, так и на других языках программирования.

В целом, Dataloader облегчает загрузку, предварительную обработку и эффективную работу с данными во время обучения нейронных сетей, что упрощает процесс разработки и улучшает производительность моделей.

<a name="_toc139242091"></a>**3.3 Аугментация данных для повышения эффективности обучения нейронной сети.**

Для достижения высокой производительности моделей требуется большой объем размеченных данных. В реальных сценариях часто бывает сложно собрать достаточное количество данных для обучения модели. В таких случаях аугментация данных становится полезным инструментом, который позволяет увеличить эффективность обучения моделей глубокого обучения.

Аугментация данных – это процесс искусственного увеличения разнообразия обучающего набора данных путем применения различных трансформаций к существующим изображениям. Это может включать изменение размера, повороты, сдвиги, отражения, изменение яркости и контраста и другие операции, которые сохраняют семантику изображения, но вносят некоторое разнообразие.

Преимущества аугментации данных:

1\. Увеличение разнообразия данных. Аугментация данных позволяет создать больше различных вариаций изображений, что помогает модели обучаться на более широком спектре данных;

2\. Регуляризация модели. Аугментация данных может помочь уменьшить переобучение путем введения случайных изменений в обучающие данные, что помогает модели обобщать лучше на новых данных;

3\. Устойчивость к шуму. Аугментация данных может улучшить устойчивость модели к шуму и искажениям, поскольку модель будет обучаться на изображениях с различными искажениями.

PyTorch предоставляет мощные инструменты для реализации аугментации данных. Один из таких инструментов – класс torchvision.transforms из библиотеки PyTorch. Он предоставляет различные функции преобразования изображений, которые могут быть использованы для аугментации данных. Например, функция RandomResizedCrop случайным образом обрезает и изменяет размер изображения до заданного размера, а функция ColorJitter изменяет яркость, контраст, насыщенность и оттенок изображения. Эти функции могут быть комбинированы и применены в цепочке, чтобы создать разнообразные преобразования и увеличить разнообразие данных для обучения моделей глубокого обучения.

На рисунке 3.3 представлено оригинальное изображение и его вариации с примененными случайным образом функциями  преобразования размера, изменения яркости, контраста, насыщенности и оттенка.![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.016.png)

Рисунок 3.3. Пример аугментации данных набора FOOD-101

Таким образом, расширим исходный набор обучающих данных дополнительными изображениями. Соответствующий код представлен на рисунке 3.4.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.017.png)

Рисунок 3.4. Аугментация данных 

<a name="_toc139242092"></a>**ГЛАВА 4. ОБУЧЕНИЕ НЕЙРОННОЙ СЕТИ ДЛЯ КЛАССИФИКАЦИИ БЛЮД. ОЦЕНКА РЕЗУЛЬТАТОВ**

<a name="_toc139242093"></a>**4.1 Перенос обучения**

Перенос обучения – это технология машинного обучения, которая позволяет использовать предобученные модели для решения новых задач.

Вместо того чтобы обучать новую модель с нуля, используется уже ранее обученная модель, которая может решать сходные задачи. Изменяется только выходной слой модели, чтобы адаптировать ее к новой задаче. Переобучение на новых данных может занять значительно меньше времени, чем обучение модели с нуля, поскольку большая часть модели уже обучена и готова к использованию.

Перенос обучения используется в тех областях, где есть необходимость классифицировать большое количество данных. Например, в задачах классификации изображений, где мы можем использовать предобученную модель для классификации новых изображений. Подобно этому, можно использовать уже обученные модели для классификации данных о продуктах или для анализа текстовых данных.

Основное преимущество переноса обучения заключается в экономии времени и ресурсов, необходимых для обучения новой модели. Кроме того, перенос обучения может значительно повысить точность классификации, поскольку уже обученная модель содержит знания, которые можно использовать для решения новых задач. В целом, перенос обучения – это технология, которая может существенно сократить время и ресурсы, необходимые для обучения новых моделей на больших объемах данных. 

В ходе исследования было проведено сравнение различных предобученных моделей, таких как DenseNet121, ResNet50 и InceptionV3, которые предварительно обучены на обширном наборе данных ImageNet и достигли впечатляющих результатов в классификации изображений. 

Однако, при использовании этих моделей в данной работе, было обнаружено, что время, затрачиваемое на одну эпоху обучения, значительно увеличивается. Это может быть связано с более сложной архитектурой данных моделей и большим количеством обучаемых параметров, что требует большего времени для вычислений.

В связи с этим, для оптимизации данной исследовательской работы было принято решение выбрать сеть ResNet18. Эта модель представляет собой более легкую версию оригинальной архитектуры ResNet, имеющую меньшее количество слоев и параметров. Такой выбор обусловлен необходимостью снижения времени обучения модели, при этом сохраняя достаточную точность классификации изображений.

Таким образом, выбор сети ResNet18 является оптимальным для данной исследовательской работы, позволяя достичь приемлемого баланса между временем обучения и точностью классификации изображений блюд.

Сеть ResNet18 была разработана в 2015 году и представляет собой упрощенную версию оригинальной сети ResNet, ее основным преимуществом является использование блоков с пропусками, известных как "связи-с-пропусками" или "shortcut connections". Эти блоки позволяют обучаемым слоям получать информацию не только от предыдущих слоев, но и напрямую от более ранних слоев. Такой подход помогает сети избежать проблемы затухания градиента и позволяет успешно обучать глубокие модели.

Архитектура ResNet18 состоит из 18 слоев, включая сверточные слои, пулинг, полносвязные слои и слои активации. Она имеет два основных типа блоков: базовый блок и блок с увеличением размерности. Базовый блок состоит из двух сверточных слоев с фильтрами размером 3x3 и функцией активации ReLU. Блок с увеличением размерности используется для изменения размерности тензора и увеличения глубины признаков.

Для использования предобученной сети ResNet18 в нашей задаче классификации с 101 классом, мы должны изменить последний полносвязный слой на новый слой с 101 выходом. Это связано с тем, что предобученная сеть была обучена на другом наборе данных, вероятно с другим количеством классов.

Кроме того, нам потребуется выбрать оптимизатор и критерий для обучения модели. В данном случае, мы можем использовать оптимизатор Adam, который является популярным выбором для обучения нейронных сетей. Оптимизатор Adam сочетает в себе методы адаптивного градиентного спуска и коррекции импульса для эффективной оптимизации модели.

Для критерия (или функции потерь) мы можем использовать CrossEntropyLoss. CrossEntropyLoss является одним из наиболее распространенных критериев для задач классификации с множеством классов. Он автоматически применяет функцию Softmax к выходам модели и вычисляет кросс-энтропийную потерю между предсказанными вероятностями классов и истинными метками классов.

Таким образом, для использования предобученной сети ResNet18 в нашей задаче, мы должны изменить последний полносвязный слой на слой с 101 выходом, выбрать оптимизатор Adam и критерий CrossEntropyLoss для обучения модели.

Описанный алгоритм представлен на рисунке 4.1.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.018.png)

Рисунок 4.1. Построение модели

В итоге, выбор сети ResNet18 для данной исследовательской работы обусловлен ее высокой эффективностью, способностью успешно обучаться на наборе данных ImageNet и возможностью достичь приемлемых результатов в классификации изображений блюд при более низком времени обучения.

<a name="_toc139242094"></a>**4.2 Обучение нейронной сети**

В процессе обучения нейронной сети используются различные параметры, которые влияют на процесс обучения.

Одним из таких параметров является эпоха, которая представляет собой прохождение всего тренировочного набора данных через нейронную сеть. То есть эпоха заканчивается, когда каждый тренировочный экземпляр был просмотрен обучаемой моделью. 

Еще одним параметром является пакет (или батч), который представляет собой небольшой набор примеров тренировочных данных, на основе которых вносятся коррективы в обучаемую модель. 

Итерация определяет количество таких пакетов, которые должны пройти через модель, чтобы завершить одну эпоху. Кроме того, обучение модели нейронной сети можно остановить, когда достигнута требуемая точность или когда ошибка нейронной сети перестала уменьшаться. 

Важно учитывать все эти параметры при обучении нейронной сети, чтобы достичь наилучших результатов.

Исследуемая в данной работе модель классификации изображений блюд была обучена с использованием Google Colab Notebook – сервиса, который представляет собой интерактивную среду на языке Python. Google Colab предоставляет бесплатный доступ к графическим процессорам и позволяет выполнять код, создавать и обучать модели машинного обучения.

Спроектированная модель была обучена на 16 эпохах, перестала улучшаться. Это означает, что дальнейшее обучение модели не приводит к существенному повышению ее способности делать правильные предсказания или улучшению ее общей производительности. 

Точность модели – это метрика, которая измеряет, насколько точно модель делает предсказания. В процессе обучения модель стремится минимизировать ошибки и максимизировать точность, чтобы достичь наилучших результатов. Однако, после некоторого количества эпох обучения, модель может достичь своего предельного предсказательного потенциала и перестать значительно улучшаться.

Такое явление называется сходимостью модели. Сходимость означает, что модель достигла оптимальных значений своих параметров и дальнейшее обучение не приводит к заметному повышению точности. Это может быть связано с насыщением данных, ограничениями архитектуры модели или недостаточным объемом данных для обучения.

Поэтому, в данном случае, модель была остановлена после 16 эпох, так как дальнейшее обучение не приводило к значимому улучшению точности модели.

Результаты обучения построенной модели представлены на рисунке 4.2. Изменения значений точности и потерь модели на каждой эпохе графически представлены на рисунке 4.3.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.019.png)

Рисунок 4.2. Результаты обучения

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.020.png)

Рисунок 4.3. Графики обучения

<a name="_toc139242095"></a>**4.3 Оценка результатов классификации**

Для оценки качества классификации были применены различные метрики оценки, которые позволяют оценить эффективность работы моделей.

В результате проведенной классификации были получены следующие оценки по метрикам:

- Accuracy: 0.8177

Данная метрика измеряет общую точность классификации и показывает, насколько хорошо модель правильно классифицировала изображения. В данном случае, полученное значение accuracy составляет 81.77%, что говорит о достаточно высокой точности классификации.

- Macro Precision: 0.8175

Данная метрика измеряет среднюю точность классификации по всем классам. Полученное значение macro precision также составляет 81.75%, что указывает на хорошую точность классификации для каждого класса в отдельности.

- Macro Recall: 0.8177

Метрика macro recall измеряет среднюю полноту классификации по всем классам. Значение macro recall составляет 81.77%, что свидетельствует о том, что модель способна правильно обнаруживать объекты каждого класса.

- Macro F1-Score: 0.8169

Данная метрика представляет собой среднее гармоническое между точностью и полнотой классификации. Полученное значение macro F1-score составляет 81.69%, что указывает на хороший баланс между точностью и полнотой классификации для всех классов.

Micro Precision, Micro Recall, Micro F1-Score: все три метрики имеют одинаковые значения, равные 0.8177. Эти метрики учитывают общее количество верно классифицированных и неверно классифицированных изображений для всех классов.

Кроме указанных метрик также полезно рассмотреть точность классификации по отдельным классам. Это позволяет оценить процент правильно классифицированных изображений для каждого класса относительно общего количества изображений этого класса.

Распределение точности по классам предоставляет информацию о способности модели правильно классифицировать каждый класс отдельно. Она позволяет выявить классы, для которых модель может иметь проблемы с классификацией, и позволяет сравнивать точность между разными классами. Подобное распределение для полученной в ходе работы модели представлено на рисунке 4.4.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.021.png)

Рисунок 4.4. Точность по классам

Для более наглядной иллюстрации результатов классификации представлены примеры верно и неверно классифицированных изображений на рисунке 4.5.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.022.png)

Рисунок 4.5. Примеры верно и неверно классифицированных изображений

Дополнительно, на рисунке 4.6 представлен пример классификации изображения из интернета, включая топ-1 и топ-5 классификации, что демонстрирует способность модели к классификации изображений в реальном мире.

![](Aspose.Words.ab79e55d-deab-47b1-9f01-2bfc89117a6a.023.png)

Рисунок 4.6. Классификация макаронс

В целом, полученные результаты классификации свидетельствуют о высокой точности и эффективности модели, а также о ее способности правильно распознавать и классифицировать объекты различных классов.

<a name="_toc123170984"></a><a name="_toc135480013"></a><a name="_toc138774247"></a><a name="_toc139242096"></a>**ЗАКЛЮЧЕНИЕ**

<a name="_toc123170985"></a>В данной выпускной квалификационной работе был исследован вопрос классификации изображений блюд. В ходе работы был проведен обзор существующих методов классификации изображений, выделены их преимущества и недостатки. На основе полученных выводов был выбран и реализован алгоритм на основе сверточных нейронных сетей, способный распознавать 101 класс объектов, указанных в выбранном для обучения наборе данных FOOD-101.

В теоретической части работы были рассмотрены основные принципы работы искусственных нейронных сетей, а также особенности их построения. Были изучены методы обучения нейронных сетей, необходимые для эффективной классификации изображений. Особое внимание было уделено применению сверточных нейронных сетей для решения задачи классификации изображений.

Для ускорения процесса обучения была выбрана предобученная нейронная сеть ResNet18. В результате проведенного обучения полученная нейронная сеть демонстрирует точность классификации изображений на уровне 81.77%.

Таким образом, данная работа представляет собой исследование и реализацию алгоритма классификации изображений блюд с использованием сверточных нейронных сетей. Полученные результаты свидетельствуют о достижении приемлемой точности классификации, что подтверждает эффективность выбранного подхода. 

Дальнейшее развитие данного исследования может включать несколько направлений для улучшения точности классификации. Например, можно рассмотреть возможность расширения набора классов объектов. В текущей работе был рассмотрен 101 класс объектов, но существует множество других блюд, которые могут быть интересны для классификации. Добавление новых классов позволит расширить область применения алгоритма и сделать его более универсальным.

Также, для практического применения в сфере распознавания блюд, алгоритм классификации изображений может потребовать адаптации. Это может включать оптимизацию алгоритма для работы в реальном времени, учет особенностей конкретных условий и требований задачи, а также интеграцию с другими системами или устройствами.

Таким образом, дальнейшее развитие данного исследования может включать улучшение точности классификации, расширение набора классов объектов и адаптацию алгоритма для практического применения в реальных проектах. Эти шаги позволят улучшить результаты и сделать разработанный алгоритм полезным и эффективным в реальных условиях.



<a name="_toc138774248"></a><a name="_toc139242097"></a>**СПИСОК ЛИТЕРАТУРЫ**

1\. Rahul Paul Classifying Cooking Object’s State using a Tuned VGG Convolutional Neural Network // Department of Computer Science and Engineering, University of South Florida, Tampa, Florida, USA. – 2018

2\. Rutuja Bhise, Prajakta Dhanba, Renuka Kshirsagar CLASSIFICATION OF THAI FAST FOOD USING CONVOLUTIONAL NEURAL NETWORK // International Journal of Innovative Research in Engineering & Multidisciplinary Physical Sciences. – 2019. – №7

3\. Bossard, L., Guillaumin, M., & Van Gool, L. (2014). Food-101–Mining Discriminative Components with Random Forests. European Conference on Computer Vision, 446-461.

4\. Mitchell T. Machine Learning / Tom Mitchell – Mc Graw Hill India. – 2017. – Mar. – С. 432.

5\. Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.

6\. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.

7\. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105).

8\. Simonyan, K., & Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.

9\. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., ... & Berg, A. C. (2015). ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3), 211-252.

10\. Ховард Джереми, Гуггер Сильвейн Глубокое обучение с fastai и PyTorch: минимум формул, минимум кода, максимум эффективности. — СПб.: Питер, 2022. — 624 с.: ил. — (Серия «Бестселлеры O’Reilly»).

11\. Гелиг А., Матвеев А. Введение в математическую теорию обучаемых распознающих систем и нейронных сетей. Учебное пособие / Аркадий Гелиг, Алексей Матвеев – Издательство СПбГУ, 2014. – 224 с

12\. Корсунов Н.И., Торопчин Д.А. Метод классификации изображений на основе кластеризации сложных объектов // Экономика. Информатика. 2016. №23 (244). URL: https://cyberleninka.ru/article/n/metod-klassifikatsii-izobrazheniy-na-osnove-klasterizatsii-slozhnyh-obektov (дата обращения: 27.06.2023).

13\. Rashid T. Make Your Own Neural Network / Tariq Rashid – CreateSpace Independent Publishing Platform. – 2016. – С. 222.

14\. А.В. Кугаевских, Д.И. Муромцев, О.В. Кирсанова. Классические методы машинного обучения. – СПб: Университет ИТМО, 2022. – 53 с.

15\. Гудфеллоу Я., Бенджио И., Курвилль А. Глубокое обучение / пер. с анг. А. А. Слинкина. – 2-е изд., испр. – М.: ДМК Пресс, 2018. – 652 с.: цв. ил.

16\. Бурков А. Машинное обучение без лишних слов / Андрей Бурков – Питер СПб, 2020. – 192 c.

3

